激活函数的作用是： 增加拟合非线性特征的能力， 因为现实当中很多特征并不是线性的。
逻辑回归解决的是分类问题

code：
import numpy as np
from sklearn.datasets import load_boston
# from sklearn.utils import shuff

# 数据加载
data = load_boston()
X_ = data['data']
y = data['target']
# 将y转化为矩阵的形式
y = y.reshape(y.shape[0],1)

# 数据规范化
X_ = (X_ - np.mean(X_, axis=0)) / np.std(X_, axis=0)

"""
    初始化网络参数
    定义隐藏层维度，w1,b1,w2,b2
""" 
n_features = X_.shape[1]
n_hidden = 10
w1 = np.random.randn(n_features, n_hidden)
b1 = np.zeros(n_hidden)
w2 = np.random.randn(n_hidden, 1)
b2 = np.zeros(1)

# relu函数
def Relu(x):
    result = np.where(x<0, 0, x)
    return result


# 设置学习率
learning_rate = 1e-6

# 定义损失函数
def MSE_loss(y, y_hat):
    return np.mean(np.square(y_hat - y))

# 定义线性回归函数
def Linear(X, w1, b1):
    y = X.dot(w1) + b1
    return y

# 5000次迭代
for t in range(5000):
    # 前向传播，计算预测值y (Linear->Relu->Linear)
    """ 这里写你的代码 """
    l1 = Linear(X_, w1, b1)
    s1 = Relu(l1)
    y_pred = Linear(s1, w2, b2)


    # 计算损失函数, 并输出每次epoch的loss
    """ 这里写你的代码 """
    loss = MSE_loss(y, y_pred)
    print(t, loss)


    # 反向传播，基于loss 计算w1和w2的梯度
    """ 这里写你的代码 """
    grad_y_pred = 2.0 * (y_pred - y)
    grad_w2 = s1.T.dot(grad_y_pred)
    grad_temp_relu = grad_y_pred.dot(w2.T)
    grad_temp_relu[l1<0] = 0
    grad_w1 = X_.T.dot(grad_temp_relu)



    # 更新权重, 对w1, w2, b1, b2进行更新
    """ 这里写你的代码 """    
    w1 = w1 - learning_rate * grad_w1
    w2 = w2 - learning_rate * grad_w2
# 得到最终的w1, w2
print('w1={} \n w2={}'.format(w1, w2))
